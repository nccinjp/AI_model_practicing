{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df1b817f",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc91e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "CLASSES = [ \"Bull\" , \"Bear\"  ]  \n",
    "LABEL_BULL = CLASSES.index( \"Bull\" )\n",
    "LABEL_BEAR = CLASSES.index( \"Bear\" )\n",
    "\n",
    "datasets = np.load(\"datasets.npz\")\n",
    "x_train, y_train = datasets[\"x_train\"],datasets[\"y_train\"]\n",
    "x_val,y_val = datasets[\"x_val\"],datasets[\"y_val\"]\n",
    "x_test,y_test = datasets[\"x_test\"],datasets[\"y_test\"]\n",
    "                                             \n",
    "# Label distribution \n",
    "label_distribution = pd.DataFrame([{\"Dataset\" : \"train\",\n",
    "                                                         \"Bull\":np.count_nonzero(y_train == LABEL_BULL),\n",
    "                                                         \"Bear\": np.count_nonzero(y_train == LABEL_BEAR)},\n",
    "                                                         {\"Dataset\" : \"val\",\n",
    "                                                         \"Bull\":np.count_nonzero(y_val == LABEL_BULL),\n",
    "                                                         \"Bear\": np.count_nonzero(y_val == LABEL_BEAR)},\n",
    "                                                         {\"Dataset\" : \"test\",\n",
    "                                                         \"Bull\":np.count_nonzero(y_test == LABEL_BULL),\n",
    "                                                         \"Bear\": np.count_nonzero(y_test == LABEL_BEAR)}])\n",
    "label_distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c6b3b4",
   "metadata": {},
   "source": [
    "# Construct model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551eecec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, Concatenate, \\\n",
    "                                                        BatchNormalization, Activation, \\\n",
    "                                                        Add, GlobalAveragePooling1D, Dense, Dropout\n",
    "from tensorflow.keras.models import Model \n",
    "\n",
    "def inception_module(input_tensor):\n",
    "    bottleneck = Conv1D(filters=32, kernel_size=1, padding=\"same\", activation=None,\n",
    "                       use_bias=False) (input_tensor)\n",
    "    conv3 = Conv1D(filters=32, kernel_size=3, padding=\"same\", activation=None,\n",
    "                       use_bias=False) (bottleneck)\n",
    "    conv5 = Conv1D(filters=32, kernel_size=5, padding=\"same\", activation=None,\n",
    "                       use_bias=False) (bottleneck)\n",
    "    conv7 = Conv1D(filters=32, kernel_size=7, padding=\"same\", activation=None,\n",
    "                       use_bias=False) (bottleneck)\n",
    "    mp = MaxPooling1D(pool_size=3, strides=1,padding=\"same\")(input_tensor)\n",
    "    mpbottleneck = Conv1D(filters=32, kernel_size=1, padding=\"same\",activation=None,\n",
    "                         use_bias=False)(mp)\n",
    "    \n",
    "    x = Concatenate(axis=-1)([conv3,conv5,conv7, mpbottleneck])\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"LeakyReLU\")(x)\n",
    "    #原本用的是ＲＥＬＵ學界的預設選項\n",
    "    #但被驗證是有缺點的\n",
    "    return x\n",
    "\n",
    "def shortcut_layer(input_tensor1, input_tensor2):\n",
    "    shortcut = Conv1D(filters=input_tensor2.shape[-1], kernel_size=1, padding=\"same\", activation=None, use_bias=False)(input_tensor1)\n",
    "    shortcut = BatchNormalization()(shortcut)\n",
    "    \n",
    "    x = Add()([shortcut, input_tensor2])\n",
    "    x = Activation(\"LeakyReLU\")(x)  \n",
    "    return x \n",
    "\n",
    "#Build model  \n",
    "n_time_steps = x_train.shape[1]\n",
    "n_features = x_train.shape[2]\n",
    "\n",
    "input_layer = Input(shape=(n_time_steps, n_features))\n",
    "x = input_layer\n",
    "input_residual = input_layer\n",
    "\n",
    "for i in range(6):\n",
    "    x = inception_module(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    \n",
    "    if i % 3 == 2 :\n",
    "        x = shortcut_layer(input_residual, x)\n",
    "        input_residual = x\n",
    "\n",
    "x = GlobalAveragePooling1D()(x)\n",
    "output_layer = Dense(len(CLASSES), activation=\"softmax\" )(x)\n",
    "\n",
    "model = Model(inputs = input_layer, outputs= output_layer)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e0098c",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04a7b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "train_label_counts = label_distribution.iloc[0]\n",
    "class_weight = {LABEL_BULL:1.,\n",
    "                           LABEL_BEAR: train_label_counts[\"Bull\"] / train_label_counts[\"Bear\"]}\n",
    "\n",
    "reduce_lr =   ReduceLROnPlateau(monitor=\"val_loss\", factor=0.2, patience=30, min_lr=0.00001)\n",
    "model_checkpoint = ModelCheckpoint(filepath=\"best_model.hdf5\", monitor=\"val_loss\",\n",
    "                                  save_best_only=True)\n",
    "\n",
    "early_stopping = EarlyStopping(monitor=\"val_loss\", patience= 100, restore_best_weights=True)\n",
    "callbacks = [model_checkpoint, early_stopping]\n",
    "\n",
    "train_history = model.fit(x_train, to_categorical(y_train),\n",
    "                         class_weight=class_weight, \n",
    "                         validation_data=(x_val, to_categorical(y_val)),\n",
    "                         batch_size= 2048, epochs=1000, callbacks=[callbacks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6b94b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#訓練後看Training loss曲線 \n",
    "\n",
    "plt.style.use(\"seaborn-v0_8\")\n",
    "fig, axes = plt.subplots(2,1,figsize=(16,12))\n",
    "\n",
    "axes[0].set_title(\"Loss\") \n",
    "axes[0].set_yscale(\"log\")\n",
    "axes[0].plot(train_history.history[\"loss\"], label=\"Training\")\n",
    "axes[0].plot(train_history.history[\"val_loss\"], label= \"Validation\")\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].set_title(\"Accuracy\")\n",
    "axes[1].set_xlabel(\"Epoch\")\n",
    "axes[1].plot(train_history.history[\"accuracy\"], label=\"Training\")\n",
    "axes[1].plot(train_history.history[\"val_accuracy\"], label= \"Validation\")\n",
    "axes[1].legend()\n",
    "                  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
